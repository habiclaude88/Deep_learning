{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5QYLVIw3ujKEoG88fTO9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habiclaude88/Deep_learning/blob/main/Regression_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Regression with the Keras Deep Learning Library in Python**"
      ],
      "metadata": {
        "id": "eHqiSoCxTdLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "you will discover how to develop and evaluate neural network models using Keras for a regression problem."
      ],
      "metadata": {
        "id": "7pkdr9OCT0QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After completing this step-by-step tutorial, you will know:\n",
        "\n",
        "How to load a CSV dataset and make it available to Keras\n",
        "\n",
        "How to create a neural network model with Keras for a regression problem\n",
        "\n",
        "How to use scikit-learn with Keras to evaluate models using cross-validation\n",
        "\n",
        "How to perform data preparation in order to improve skill with Keras models\n",
        "\n",
        "How to tune the network topology of models with Keras"
      ],
      "metadata": {
        "id": "EDh4Rak-T-4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Description**\n",
        "\n",
        "The problem that we will look at in this tutorial is the Boston house price dataset.\n",
        "\n",
        "You can download this dataset and save it to your current working directly with the file name housing.csv (update: download data from here).\n",
        "\n",
        "The dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include crime rate, the proportion of nonretail business acres, chemical concentrations, and more.\n",
        "\n",
        "This is a well-studied problem in machine learning. It is convenient to work with because all the input and output attributes are numerical, and there are 506 instances to work with.\n",
        "\n",
        "Reasonable performance for models evaluated using Mean Squared Error (MSE) is around 20 in thousands of dollars squared (or $4,500 if you take the square root). This is a nice target to aim for with our neural network model."
      ],
      "metadata": {
        "id": "s4bTYKH6UZxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "NaqKvITNEgOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "4TpiaTIKEqDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Develop a Baseline Neural Network Model**"
      ],
      "metadata": {
        "id": "CRhgM7TZMpDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "you will create a baseline neural network model for the regression problem.\n",
        "\n",
        "Let’s start by including all the functions and objects you will need for this"
      ],
      "metadata": {
        "id": "29pKdI3fUwb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "KqQKov1fEHtz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now load your dataset from a file in the local directory.\n",
        "\n",
        "The dataset is, in fact, not in CSV format in the UCI Machine Learning Repository. The attributes are instead separated by whitespace. You can load this easily using the pandas library. Then split the input (X) and output (Y) attributes, making them easier to model with Keras and scikit-learn."
      ],
      "metadata": {
        "id": "zN6Ykk87VGJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataframe = pd.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]"
      ],
      "metadata": {
        "id": "qnuSRE9_BIbi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create Keras models and evaluate them with scikit-learn using handy wrapper objects provided by the Keras library. This is desirable, because scikit-learn excels at evaluating models and will allow you to use powerful data preparation and model evaluation schemes with very few lines of code.\n",
        "\n",
        "The Keras wrappers require a function as an argument. This function you must define is responsible for creating the neural network model to be evaluated.\n",
        "\n",
        "Below, you will define the function to create the baseline model to be evaluated. It is a simple model with a single, fully connected hidden layer with the same number of neurons as input attributes (13). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem, and you are interested in predicting numerical values directly without transformation.\n",
        "\n",
        "The efficient ADAM optimization algorithm is used, and a mean squared error loss function is optimized. This will be the same metric you will use to evaluate the performance of the model. It is a desirable metric because taking the square root gives an error value you can directly understand in the context of the problem (thousands of dollars)."
      ],
      "metadata": {
        "id": "wRBspPYuVOIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "zTosClXDCOyZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras wrapper object used in scikit-learn as a regression estimator is called KerasRegressor. You create an instance and pass it both the name of the function to create the neural network model and some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size. Both of these are set to sensible defaults.\n",
        "\n",
        "The final step is to evaluate this baseline model. You will use 10-fold cross validation to evaluate the model."
      ],
      "metadata": {
        "id": "v5G4NbiFVW5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = KerasRegressor(model=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P83PKaScJZIu",
        "outputId": "123de363-caf5-4e62-b6ea-8bed4af27837"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: -37.00 (26.92) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Example With Boston Dataset: Baseline\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model\n",
        "estimator = KerasRegressor(model=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHUeXJv4JgAR",
        "outputId": "55bcd2b0-57e3-4aab-983b-897f690299de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: -32.26 (27.19) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this code gives you an estimate of the model’s performance on the problem for unseen data.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "Note: The mean squared error is negative because scikit-learn inverts so that the metric is maximized instead of minimized. You can ignore the sign of the result.\n",
        "\n",
        "The result reports the mean squared error, including the average and standard deviation (average variance) across all ten folds of the cross validation evaluation."
      ],
      "metadata": {
        "id": "-6ycdbLWViw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Modeling the Standardized Dataset**"
      ],
      "metadata": {
        "id": "O5q3rcUOMz8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Tutorial with the Keras Deep Learning Library in Python\n",
        "by Jason Brownlee on June 9, 2016 in Deep Learning\n",
        "Tweet Tweet  Share\n",
        "Last Updated on August 5, 2022\n",
        "\n",
        "Keras is a deep learning library that wraps the efficient numerical libraries Theano and TensorFlow.\n",
        "\n",
        "In this post, you will discover how to develop and evaluate neural network models using Keras for a regression problem.\n",
        "\n",
        "After completing this step-by-step tutorial, you will know:\n",
        "\n",
        "How to load a CSV dataset and make it available to Keras\n",
        "How to create a neural network model with Keras for a regression problem\n",
        "How to use scikit-learn with Keras to evaluate models using cross-validation\n",
        "How to perform data preparation in order to improve skill with Keras models\n",
        "How to tune the network topology of models with Keras\n",
        "Kick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n",
        "\n",
        "Let’s get started.\n",
        "\n",
        "Jun/2016: First published\n",
        "Update Mar/2017: Updated for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0\n",
        "Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down\n",
        "Update Apr/2018: Changed nb_epoch argument to epochs\n",
        "Update Sep/2019: Updated for Keras 2.2.5 API\n",
        "Update Jul/2022: Update for TensorFlow 2.x syntax with SciKeras\n",
        "Regression Tutorial with Keras Deep Learning Library in Python\n",
        "Regression tutorial with Keras deep learning library in Python\n",
        "Photo by Salim Fadhley, some rights reserved.\n",
        "\n",
        "1. Problem Description\n",
        "The problem that we will look at in this tutorial is the Boston house price dataset.\n",
        "\n",
        "You can download this dataset and save it to your current working directly with the file name housing.csv (update: download data from here).\n",
        "\n",
        "The dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include crime rate, the proportion of nonretail business acres, chemical concentrations, and more.\n",
        "\n",
        "This is a well-studied problem in machine learning. It is convenient to work with because all the input and output attributes are numerical, and there are 506 instances to work with.\n",
        "\n",
        "Reasonable performance for models evaluated using Mean Squared Error (MSE) is around 20 in thousands of dollars squared (or $4,500 if you take the square root). This is a nice target to aim for with our neural network model.\n",
        "\n",
        "Need help with Deep Learning in Python?\n",
        "Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with code).\n",
        "\n",
        "Click to sign-up now and also get a free PDF Ebook version of the course.\n",
        "\n",
        "Start Your FREE Mini-Course Now\n",
        "\n",
        "\n",
        "2. Develop a Baseline Neural Network Model\n",
        "In this section, you will create a baseline neural network model for the regression problem.\n",
        "\n",
        "Let’s start by including all the functions and objects you will need for this tutorial.\n",
        "\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "...\n",
        "You can now load your dataset from a file in the local directory.\n",
        "\n",
        "The dataset is, in fact, not in CSV format in the UCI Machine Learning Repository. The attributes are instead separated by whitespace. You can load this easily using the pandas library. Then split the input (X) and output (Y) attributes, making them easier to model with Keras and scikit-learn.\n",
        "\n",
        "...\n",
        "# load dataset\n",
        "dataframe = pd.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "You can create Keras models and evaluate them with scikit-learn using handy wrapper objects provided by the Keras library. This is desirable, because scikit-learn excels at evaluating models and will allow you to use powerful data preparation and model evaluation schemes with very few lines of code.\n",
        "\n",
        "The Keras wrappers require a function as an argument. This function you must define is responsible for creating the neural network model to be evaluated.\n",
        "\n",
        "Below, you will define the function to create the baseline model to be evaluated. It is a simple model with a single, fully connected hidden layer with the same number of neurons as input attributes (13). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem, and you are interested in predicting numerical values directly without transformation.\n",
        "\n",
        "The efficient ADAM optimization algorithm is used, and a mean squared error loss function is optimized. This will be the same metric you will use to evaluate the performance of the model. It is a desirable metric because taking the square root gives an error value you can directly understand in the context of the problem (thousands of dollars).\n",
        "\n",
        "If you are new to Keras or deep learning, see this Keras tutorial.\n",
        "\n",
        "...\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "The Keras wrapper object used in scikit-learn as a regression estimator is called KerasRegressor. You create an instance and pass it both the name of the function to create the neural network model and some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size. Both of these are set to sensible defaults.\n",
        "\n",
        "The final step is to evaluate this baseline model. You will use 10-fold cross validation to evaluate the model.\n",
        "\n",
        "...\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
        "After tying this all together, the complete example is listed below.\n",
        "\n",
        "# Regression Example With Boston Dataset: Baseline\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model\n",
        "estimator = KerasRegressor(model=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
        "Running this code gives you an estimate of the model’s performance on the problem for unseen data.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "Note: The mean squared error is negative because scikit-learn inverts so that the metric is maximized instead of minimized. You can ignore the sign of the result.\n",
        "\n",
        "The result reports the mean squared error, including the average and standard deviation (average variance) across all ten folds of the cross validation evaluation.\n",
        "\n",
        "Baseline: -32.65 (23.33) MSE\n",
        "\n",
        "3. Modeling the Standardized Dataset\n",
        "An important concern with the Boston house price dataset is that the input attributes all vary in their scales because they measure different quantities.\n",
        "\n",
        "It is almost always good practice to prepare your data before modeling it using a neural network model.\n",
        "\n",
        "Continuing from the above baseline model, you can re-evaluate the same model using a standardized version of the input dataset.\n",
        "\n",
        "You can use scikit-learn’s Pipeline framework to perform the standardization during the model evaluation process within each fold of the cross validation. This ensures that there is no data leakage from each test set cross validation fold into the training data.\n",
        "\n",
        "The code below creates a scikit-learn pipeline that first standardizes the dataset and then creates and evaluates the baseline neural network model."
      ],
      "metadata": {
        "id": "Dm3q-PrAVpNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NKJ4Tu1KV6P",
        "outputId": "b5dac1a8-0f0a-42f5-86d9-e982b8e43178"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized: -30.84 (28.24) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Example With Boston Dataset: Standardized\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUDf6Wn5NUtC",
        "outputId": "ee684ee5-3cdc-4c16-c28a-7cc38ca0b669"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized: -27.82 (24.87) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "Running the example provides an improved performance over the baseline model without standardized data, dropping the error."
      ],
      "metadata": {
        "id": "YLROuRJWV2RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A further extension of this section would be to similarly apply a rescaling to the output variable, such as normalizing it to the range of 0-1 and using a Sigmoid or similar activation function on the output layer to narrow output predictions to the same range."
      ],
      "metadata": {
        "id": "INm-K_whWDHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Tune the Neural Network Topology**"
      ],
      "metadata": {
        "id": "q2HifR9vP9k4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many concerns can be optimized for a neural network model.\n",
        "\n",
        "Perhaps the point of biggest leverage is the structure of the network itself, including the number of layers and the number of neurons in each layer.\n",
        "\n",
        "you will evaluate two additional network topologies in an effort to further improve the performance of the model. You will look at both a deeper and a wider network topology.\n",
        "\n",
        "Evaluate a Deeper Network Topology\n",
        "\n",
        "One way to improve the performance of a neural network is to add more layers. This might allow the model to extract and recombine higher-order features embedded in the data.\n",
        "\n",
        "you will evaluate the effect of adding one more hidden layer to the model. This is as easy as defining a new function to create this deeper model, copied from your baseline model above. You can then insert a new line after the first hidden layer—in this case, with about half the number of neurons."
      ],
      "metadata": {
        "id": "ZKxr8JpMWLk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "VWHu2GkBN4Yq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can evaluate this network topology in the same way as above, while also using the standardization of the dataset shown above to improve performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "AbOzaH7dWcXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-QVgqLQF0_",
        "outputId": "d40b63e6-33ec-452c-ad01-01e03f4465dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Larger: -24.06 (26.84) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "Running this model shows a further improvement in performance from 28 down to 24 thousand squared dollars."
      ],
      "metadata": {
        "id": "K4qa0TQCWiq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Example With Boston Dataset: Standardized and Larger\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define the model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nugFU4yQPF3",
        "outputId": "db419100-f200-4cad-d64f-bd877b62adec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Larger: -22.73 (24.95) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Evaluate a Wider Network Topology**"
      ],
      "metadata": {
        "id": "Ae6Ogc7TRO6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach to increasing the representational capability of the model is to create a wider network.\n",
        "\n",
        "In this section, you will evaluate the effect of keeping a shallow network architecture and nearly doubling the number of neurons in the one hidden layer.\n",
        "\n",
        "Again, all you need to do is define a new function that creates your neural network model. Here, you will increase the number of neurons in the hidden layer compared to the baseline model from 13 to 20."
      ],
      "metadata": {
        "id": "M1b7YFL-Ww-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define wider model\n",
        "def wider_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "T1C3YS9UQtWl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can evaluate the wider network topology using the same scheme as above:"
      ],
      "metadata": {
        "id": "5DLvEMx1XELK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRPOFiooRpjZ",
        "outputId": "3385a55c-6f2f-45f6-aa8d-40db47ad9d3e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wider: -22.92 (23.93) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "Building the model reveals a further drop in error to about 21 thousand squared dollars. This is not a bad result for this problem."
      ],
      "metadata": {
        "id": "E7Ikl0X6XLSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Example With Boston Dataset: Standardized and Wider\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define wider model\n",
        "def wider_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpWPWu2TRuR3",
        "outputId": "4c14c255-6d3f-494a-fc8a-a384eb352422"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wider: -22.58 (24.47) MSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It might have been hard to guess that a wider network would outperform a deeper network on this problem. The results demonstrate the importance of empirical testing in developing neural network models.\n",
        "\n"
      ],
      "metadata": {
        "id": "7hibCPMjXP_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Through this notebook, you learned how to develop and evaluate neural network models, including:**\n",
        "\n",
        "How to load data and develop a baseline model\n",
        "How to lift performance using data preparation techniques like standardization\n",
        "How to design and evaluate networks with different varying topologies on a problem"
      ],
      "metadata": {
        "id": "LMp-vhwNXaq8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7nzSGZCSftA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}